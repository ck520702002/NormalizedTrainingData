# NormalizedTrainingData
As previous works have shown that, normalization of training data enjoys both the effi- ciency of space and learning time while preserving the same performance under certain models (e.g. linear regression) and data conditions (Schleich et al., 2016). However, real-world data are often subject to noises, where exact normalization as used in the previous work couldn’t be obtained. Though this problem can be resolved by adopting approximate normalization (Kenig et al., 2019), it’s unclear whether the performance of the model will be still preserved, assuming the speed-up holds true for approximate normalized schemes. In this project, the performance of the learning result over ap- proximate normalization, as a substitute for exact normalization, is examined to explore the benefits/ losses other than the speed-up of training linear regression models over normalized schemes in real-world data-sets.
